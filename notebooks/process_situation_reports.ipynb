{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6fb2107-24f1-4355-9b1f-157915d02dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from textacy import extract\n",
    "\n",
    "from collections import defaultdict \n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dddbe2-cc32-4ea6-90a4-e5d54c109b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2023, tm_mon=12, tm_mday=2, tm_hour=5, tm_min=55, tm_sec=30, tm_wday=5, tm_yday=336, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc9b065-6c1f-44ff-a53c-9a79e3f1a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files\n",
    "pcode_file = \"D://projects//_external_files//cod_files//combined_locations//locations.csv\"\n",
    "situation_reports = \"D://projects//_external_files//surveyor//rw_siturep_preprocessed//reliefweb_situation_reports.xlsx\"\n",
    "situation_reports = \"D://projects//_external_files//surveyor//rw_disaster_preprocessed//disaster_summaries_2f96ea6d16c942018c0ac2469aab62a3.xlsx\"\n",
    "\n",
    "#situation_reports = \"D:\\projects\\_external_files\\reliefweb_disaster_reports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b94269-2fda-432c-bd1d-1c5097f2be43",
   "metadata": {},
   "source": [
    "## Load geolocation_services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cfd8aa-4f25-4857-abad-ba987499fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = pd.read_csv(pcode_file)\n",
    "\n",
    "def get_pcode_from_location(loc, country_prefix='XX', lang_code='all'):\n",
    "\n",
    "    if country_prefix != 'XX': #if the country prefix is set, limit search to that\n",
    "        df_loc = df_location[df_location['pcode_prefix'] == country_prefix]\n",
    "    else:\n",
    "        df_loc = df_location\n",
    "\n",
    "    if lang_code != 'all': #secondary filter - especially important to remove dupes with diff langs share the same script\n",
    "        df_loc = df_loc[df_loc['lang_code'] == lang_code]\n",
    "        \n",
    "    matches = df_loc['pcode'][df_loc['location_name'].str.lower() == loc.lower()].tolist()\n",
    "\n",
    "    #if the match fails, try again on the normalized name\n",
    "    if len(matches) == 0:\n",
    "        #remove common variations in names that can cause misses\n",
    "        n_loc = re.sub(r'[^a-zA-Z]', '', loc)\n",
    "\n",
    "        #this will cause problems for non-English.. so if then len is 0, exit\n",
    "        if len(n_loc) == 0:\n",
    "            return []\n",
    "            \n",
    "        matches = df_loc['pcode'][df_loc['location_normalized'].str.lower() == n_loc.lower()].tolist()\n",
    "        \n",
    "\n",
    "    #now check results\n",
    "    if len(matches) > 1:\n",
    "        #print(f\"more than 1 matches... likely due to different granularity of entities with the same name (ie. Herat City in Herat Province) {matches}\")\n",
    "        #print(f\"returning the lowest granularity match. {min(matches, key=len)}\")\n",
    "        #print(\"if the pcodes are all the same granularity.... you get the first element.\")\n",
    "        return min(matches, key=len)\n",
    "            \n",
    "        return matches\n",
    "    elif len(matches) == 1:\n",
    "        return matches\n",
    "\n",
    "    else:\n",
    "        #couldn't find a match, do a fuzzy search\n",
    "        compare_list = list(set(df_loc['location_name'].tolist()))\n",
    "        possible_matches=[]\n",
    "        for i in compare_list:\n",
    "            if fuzz.ratio(loc,i) > 70:\n",
    "                possible_matches.append(i)\n",
    "                print (f\"No exact match to '{loc}'. see if these alternative spellings are correct: {possible_matches}\")\n",
    "\n",
    "    \n",
    "    return []\n",
    "\n",
    "assert get_pcode_from_location('istanbul')[0] == 'TUR034'\n",
    "\n",
    "def get_adm_lvl_from_pcode(pcode):\n",
    "    return list(set(df_location['adm_lvl'][df_location['pcode'] == pcode].tolist()))\n",
    "    \n",
    "def get_name_in_lang(pcode, lang='en'):\n",
    "    return list(set(df_location['location_name'][(df_location['pcode'] == pcode) & (df_location['lang_code'] == lang)].tolist()))\n",
    "\n",
    "def get_descendents_of(pcode, lang='en', include_self=True):\n",
    "    if include_self==True:\n",
    "        return df_location[df_location['pcode'].str.contains(pcode) & (df_location['lang_code'] == lang)]\n",
    "    else:\n",
    "        return df_location[df_location['pcode'].str.contains(pcode) & (df_location['lang_code'] == lang)\\\n",
    "        & (df_location['pcode'] != pcode)]\n",
    "\n",
    "def get_admin_chain(pcode, lang='en'):\n",
    "    split_pcode = df_location['split_pcode'][df_location['pcode'] == pcode].tolist()[0]\n",
    "    levels = split_pcode.split(\".\")\n",
    "    pc =''\n",
    "    admin_chain = []\n",
    "    #rebuild the pcode one level at a time\n",
    "    for i in levels:\n",
    "        pc = pc + i\n",
    "        admin_chain.append(df_location['location_name'][(df_location['pcode'] == pc) & (df_location['lang_code'] == lang)].tolist()[0])\n",
    "\n",
    "    return admin_chain\n",
    "\n",
    "def get_all_locations(lang_code='all'):\n",
    "\n",
    "    #return all unique location names\n",
    "    if lang_code == 'all':\n",
    "        return list(set(df_location['location_name'].to_list()))\n",
    "    else:\n",
    "        return list(set(df_location['location_name'][df_location['lang_code'] == lang_code].to_list()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8734f6d-b090-454c-aad7-fda7d38d0b12",
   "metadata": {},
   "source": [
    "## Load Preprocessing Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf238f1-4813-4690-9dcc-c7d9c704364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(text):\n",
    "    \n",
    "    def convert_spelled_nums_to_digit(token):\n",
    "        clean_token = re.sub(r'[^a-zA-Z]', '', token).lower()\n",
    "        \n",
    "        mappings = {\n",
    "            'one' : 1,'two' : 2,'three' : 3,'four' : 4,'five' : 5,'six' : 6,'seven' : 7,'eight' : 8,'nine' : 9, 'ten' : 10\n",
    "            ,'eleven' : 11, 'twelve' : 12, 'thirteen':13, 'fourteen':14, 'fifteen':15, 'sixteen':16, 'seventeen':17\n",
    "            ,'eighteen':18, 'nineteen':19, 'twenty':20, 'dozen':12\n",
    "        }\n",
    "    \n",
    "        if mappings.get(clean_token) is not None:\n",
    "            return mappings[clean_token]\n",
    "        else:\n",
    "            return token\n",
    "        \n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"-\",\"_\") #replace so tokenization doesn't separate\n",
    "\n",
    "    #turn 'four' into 4\n",
    "    text = ' '.join([str(convert_spelled_nums_to_digit(t)) for t in text.split(\" \")])\n",
    "\n",
    "\n",
    "    # remove content in parentheses\n",
    "    #processed_string = re.sub(r'\\([^)]*\\)', '', input_string)\n",
    "\n",
    "    #remove all non alpha numeric and punctuation\n",
    "    pattern = r'[^a-zA-Z0-9\\s\\,\\.\\?\\!\\-\\(\\)]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    pattern = r'(\\d+)\\s+million'\n",
    "    text = re.sub(r'(\\d+)\\s+million', r'\\1,000,000', text)\n",
    "\n",
    "    \n",
    "    #remove commas that serve as thousands separators\n",
    "    #Hack... fix this so I don't have to run it 3x\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "    text = text.replace(\"\\s+\",\"\\s\")\n",
    "    return text\n",
    "\n",
    "def string_remove_parenthetical_content(text):\n",
    "    # Use regular expression to remove content inside parentheses\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff189ddf-4fc4-4d75-a881-0274b2b919ce",
   "metadata": {},
   "source": [
    "## Load NLP routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e49e55-1a83-4914-803f-6b5cdb069a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Create patterns and add to the entity ruler to better find locations\n",
    "\n",
    "all_locs = get_all_locations(lang_code='en')\n",
    "gpes = []\n",
    "\n",
    "STOP_LOCS = ['of','can']\n",
    "all_locs = [e for e in all_locs if e.lower() not in STOP_LOCS]\n",
    "\n",
    "# create pattern rules for locations based on the COD files\n",
    "for l in all_locs:\n",
    "    token_sequence=[]\n",
    "    for token in l.split('\\s+'):\n",
    "        token_sequence.append({\"LOWER\":token.lower()})\n",
    "    x = {'label':'GPE', 'pattern': token_sequence, 'id':get_pcode_from_location(l, lang_code='en')[0]}\n",
    "    gpes.append(x)\n",
    "    #print(get_pcode_from_location(l, lang_code='en'))\n",
    "\n",
    "ruler = nlp.add_pipe('entity_ruler', before='ner')\n",
    "ruler.add_patterns(gpes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8591a05-cefd-41b9-800f-4f9ac23b7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_indicators\n",
    "indicators = {\n",
    "    'i_people' : ['people','person','child','man','woman','civilian','colleague','fatality']\n",
    "    ,'i_killed' : ['dead','fatal','die','kill','deceased','fatality','fatality'] #think about how to incorporate 2 co-existing terms \"648 people who lost their lives\"\n",
    "    ,'i_injured' : ['injure','wound','wounded']\n",
    "    ,'i_damage' : ['damage','destroy','collapse']\n",
    "    ,'i_health_infrastructure' : ['hospital','surgery']\n",
    "    ,'i_education_infrastructure' : ['school','university']\n",
    "    ,'i_cash_xfer' : ['xx']\n",
    "    ,'i_wash' : ['sanitation','water','sewer','drain','drainage']\n",
    "    ,'i_shelter' : ['shelter','tent','camp','blanket']\n",
    "    ,'i_food' : ['food','cook','stove','feed','feed','nutrient','meal']\n",
    "    ,'i_health' : ['health','medical','medicine']\n",
    "    ,'i_gender_vuln' : ['dignity','gender','pregnant','lactate','lactating']\n",
    "    ,'i_protection' : ['trauma','mental']\n",
    "    ,'i_response_capacity' : ['personnel']\n",
    "    ,'i_other_infrastructure' : ['communicate','radio','internet','telecommunication','electric','line']\n",
    "    ,'i_money' : ['grant','loan','finance','appeal','chf','fund']\n",
    "    ,'i_other' : ['biometric']\n",
    "    ,'i_problem' : ['challenge']\n",
    "    ,'i_demand_side' : ['need','demand','gap','priority', 'receive'] # note receive implies both supply and demand\n",
    "    ,'i_supply_side' : ['response','contribute','provide','source','address','deploy','receive'] # note receive implies both supply and demand\n",
    "\n",
    "    ,'i_assessments' : ['assess','assessment']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f589028-1219-48ce-9e27-283ce730511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_tense_verb(doc):\n",
    "    def is_future_tense(token):\n",
    "        #Check if a token is indicative of future tense.\n",
    "        return (\n",
    "            token.tag_ == \"MD\" and token.text.lower() == \"will\"\n",
    "            or (token.dep_ == \"aux\" and token.head.lemma_ == \"will\")\n",
    "        )\n",
    "\n",
    "    for t in doc:\n",
    "        if is_future_tense(t):\n",
    "            return f\"{t.text} {t.head}\"\n",
    "\n",
    "def find_and_add_indicator(df, indicators):\n",
    "    ind_counter = []\n",
    "    for ind in indicators:\n",
    "  \n",
    "        df[ind] = df['lower_lemmas'].apply(lambda x: 1 if len([w for w in x if w in indicators[ind]])>0 else 0)\n",
    "        ind_counter.append(ind)\n",
    "        #print(ind_counter)\n",
    "    df['i_count'] = df[ind_counter].sum(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def declare_primary_record_type(row):\n",
    "\n",
    "    if row['i_count'] == 0:\n",
    "        return 'background'\n",
    "    elif row['i_supply_side']:\n",
    "        return 'response_details'\n",
    "    elif row['i_demand_side']:\n",
    "        return 'demand_side'\n",
    "    elif row[['i_damage','i_health_infrastructure','i_education_infrastructure']].sum() > 0:\n",
    "        return 'damage_to_homes_and_infrastructure'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def obtain_killed_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def reset_indicators():\n",
    "        return -1, -1, -1\n",
    "\n",
    "    noun, attribute, count = reset_indicators()\n",
    "            \n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "            print(f\"  c {t}\")\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "            print(f\"   peop {t}\")\n",
    "        if t.lemma_ in indicators['i_killed']:\n",
    "            attribute = t\n",
    "            print(f\"    kil {t}\")\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun, attribute, count = reset_indicators()\n",
    "\n",
    "    #if more than 1 figure is returned, typically those will be\n",
    "    #contextualizing numbers, just return the first\n",
    "    return just_count[0]\n",
    "\n",
    "\n",
    "def obtain_injured_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def reset_indicators():\n",
    "        return -1, -1, -1\n",
    "\n",
    "    noun, attribute, count = reset_indicators()\n",
    "            \n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "            print(f\"  c {t}\")\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "            print(f\"   peop {t}\")\n",
    "        if t.lemma_ in indicators['i_injured']:\n",
    "            attribute = t\n",
    "            print(f\"    kil {t}\")\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun, attribute, count = reset_indicators()\n",
    "\n",
    "    #if more than 1 figure is returned, typically those will be\n",
    "    #contextualizing numbers, just return the first\n",
    "    return just_count[0]\n",
    "\n",
    "\n",
    "def OLD_obtain_killed_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    #doc = doc.tolist()[0]\n",
    "    attribute = -1\n",
    "    noun = -1\n",
    "    count = -1\n",
    "\n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "        if t.lemma_ in indicators['i_killed']:\n",
    "            attribute = t\n",
    "\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun = -1\n",
    "            attribute = -1\n",
    "            count = -1\n",
    "\n",
    "    #changing to return only the count\n",
    "    return just_count\n",
    "    #return key_values\n",
    "            \n",
    "    \n",
    "def obtain_injured_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    #doc = doc.tolist()[0]\n",
    "    attribute = -1\n",
    "    noun = -1\n",
    "    count = -1\n",
    "\n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "        if t.lemma_ in indicators['i_injured']:\n",
    "            attribute = t\n",
    "\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun = -1\n",
    "            attribute = -1\n",
    "            count = -1\n",
    "\n",
    "    #changing to return only the count\n",
    "    return just_count\n",
    "    #return key_values\n",
    "\n",
    "def obtain_counted_noun_chunks(doc):\n",
    "    counted_things = []\n",
    "    for x in list(extract.noun_chunks(doc)):\n",
    "        for token in x:\n",
    "            if str(token).isdigit():\n",
    "                counted_things.append(x)\n",
    "                continue\n",
    "    if len(counted_things) > 0:\n",
    "        return counted_things\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "def obtain_all_entities(doc):\n",
    "\n",
    "    STOP_ENTS = ['WASH','PSS','GTC','PFA','NFI','IYCF']\n",
    "    STOP_ENT_TYPE = ['CARDINAL','ORDINAL']\n",
    "    #stop_ents = STOP_ENTS\n",
    "    ents = list(extract.entities(doc))\n",
    "    if len(ents) < 1:\n",
    "        return None\n",
    "    entities = defaultdict(list) \n",
    "    for e in ents:\n",
    "        #if e.text not in stop_ents:\n",
    "        if e.label_ not in STOP_ENT_TYPE:\n",
    "            entities[e.label_].append(e)\n",
    "\n",
    "    return entities   \n",
    "\n",
    "def extract_entities(row):\n",
    "    entities = row['entities']\n",
    "    if entities is None:\n",
    "        return ''\n",
    "    en=[]\n",
    "    for label in entities:\n",
    "        for e in entities.get(label):\n",
    "            ent = ' '.join([w.text for w in e]).strip()\n",
    "            en.append([label,ent])\n",
    "            \n",
    "    return en\n",
    "\n",
    "\n",
    "def extract_ncs(row):\n",
    "    \n",
    "    #data type, list of spans\n",
    "    xs = row['noun_chunks']\n",
    "    if xs is None:\n",
    "        return ''\n",
    "    en=[]\n",
    "\n",
    "    for e in xs:\n",
    "        ent = ' '.join([w.text for w in e]).strip()\n",
    "        en.append(['NOUN_CHUNK',ent])\n",
    "    return en\n",
    "\n",
    "\n",
    "def extract_numeric_key_values(row):\n",
    "    #data type, list of spans\n",
    "    xs = row['num_others']\n",
    "    if xs is None:\n",
    "        return ''\n",
    "    return_list=[]\n",
    "\n",
    "    for e in xs:\n",
    "        prefix = ''\n",
    "        numeric = ''\n",
    "        suffix = ''\n",
    "\n",
    "        for token in e:\n",
    "            if token.is_alpha == False:\n",
    "                numeric = token.text\n",
    "            elif numeric == '': #alpha but numeric not set yet, this is prefix\n",
    "                prefix = prefix + ' ' + token.text\n",
    "            else:\n",
    "                suffix = suffix + ' ' + token.text\n",
    "        \n",
    "        return_list.append([prefix.strip(),numeric,suffix.strip()])   \n",
    "        \n",
    "    return return_list\n",
    "\n",
    "\n",
    "def split_key_value_in_df(field,delim=','):\n",
    "\n",
    "    s = pd.Series({'prefix' : field, 'left_label' : field, 'right_label' : field})\n",
    "    \n",
    "    if isinstance(field, list):\n",
    "        fields = field\n",
    "    elif isinstance(field, str):\n",
    "        fields = field.split(delim)\n",
    "    else:\n",
    "        print(field)\n",
    "    \n",
    "     \n",
    "    if len(fields) == 2:\n",
    "        s = pd.Series({'prefix' : '', 'left_label' : fields[0], 'right_label' : fields[1]})\n",
    "    elif len(fields) == 3:\n",
    "        s = pd.Series({'prefix' : fields[0], 'left_label' : fields[1], 'right_label' : fields[2]})\n",
    "\n",
    "\n",
    "\n",
    "    return s\n",
    "\n",
    "def split_key_value_in_df_orig(field,left_label=\"d\",right_label=\"f\",delim=','):\n",
    "\n",
    "    s = pd.Series({left_label : field, right_label : field})\n",
    "    \n",
    "    if isinstance(field, list):\n",
    "     \n",
    "        if len(field) == 2:\n",
    "            s = pd.Series({left_label : field[0], right_label : field[1]})\n",
    "\n",
    "    elif isinstance(field, str):\n",
    "        fields = field.split(delim)\n",
    "        if len(fields) == 2:\n",
    "            s = pd.Series({left_label : fields[0], right_label : fields[1]})\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021d782-2fbb-4db6-afe1-913195073afa",
   "metadata": {},
   "source": [
    "## Now build base DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6183b2b6-a15f-4da2-b6c6-b8f17e4824d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_type</th>\n",
       "      <th>status</th>\n",
       "      <th>source_url</th>\n",
       "      <th>glide_id</th>\n",
       "      <th>source_level_country</th>\n",
       "      <th>source_title</th>\n",
       "      <th>source_desc</th>\n",
       "      <th>source_original_text</th>\n",
       "      <th>reference_url</th>\n",
       "      <th>text</th>\n",
       "      <th>authoring_org</th>\n",
       "      <th>reported_date</th>\n",
       "      <th>string_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-2013-000175-COD</td>\n",
       "      <td>DR Congo</td>\n",
       "      <td>DR Congo: Cholera and Measles Outbreaks - Jan ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>https://reliefweb.int/disaster/ep-2013-000175-cod</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-10-16T12:15:16+00:00</td>\n",
       "      <td>While the last major cholera outbreak in DR Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-2013-000175-COD</td>\n",
       "      <td>DR Congo</td>\n",
       "      <td>DR Congo: Cholera and Measles Outbreaks - Jan ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>https://reliefweb.int/disaster/ep-2013-000175-cod</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-10-16T12:15:16+00:00</td>\n",
       "      <td>intdisasterep2011000076cod), cholera is endemi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-2013-000175-COD</td>\n",
       "      <td>DR Congo</td>\n",
       "      <td>DR Congo: Cholera and Measles Outbreaks - Jan ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>https://reliefweb.int/disaster/ep-2013-000175-cod</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-10-16T12:15:16+00:00</td>\n",
       "      <td>Over the course of 2013, ongoing violence and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-2013-000175-COD</td>\n",
       "      <td>DR Congo</td>\n",
       "      <td>DR Congo: Cholera and Measles Outbreaks - Jan ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>https://reliefweb.int/disaster/ep-2013-000175-cod</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-10-16T12:15:16+00:00</td>\n",
       "      <td>Between January and September, more than 21000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EP-2013-000175-COD</td>\n",
       "      <td>DR Congo</td>\n",
       "      <td>DR Congo: Cholera and Measles Outbreaks - Jan ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>https://reliefweb.int/disaster/ep-2013-000175-cod</td>\n",
       "      <td>While the last major **cholera** outbreak in D...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-10-16T12:15:16+00:00</td>\n",
       "      <td>During the same time frame, more than 74299 ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15501</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL-2023-000236-DOM</td>\n",
       "      <td>Dominican Rep.</td>\n",
       "      <td>Dominican Republic: Floods and Landslides - No...</td>\n",
       "      <td>[]</td>\n",
       "      <td>As of 21 November, severe rains and subsequent...</td>\n",
       "      <td>https://reliefweb.int/disaster/fl-2023-000236-dom</td>\n",
       "      <td>As of 21 November, severe rains and subsequent...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-11-28T19:32:06+00:00</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15502</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL-2023-000236-DOM</td>\n",
       "      <td>Dominican Rep.</td>\n",
       "      <td>Dominican Republic: Floods and Landslides - No...</td>\n",
       "      <td>[]</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>https://reliefweb.int/disaster/fl-2023-000236-dom</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-11-28T19:32:06+00:00</td>\n",
       "      <td>According to information from 22 November, 741...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15503</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL-2023-000236-DOM</td>\n",
       "      <td>Dominican Rep.</td>\n",
       "      <td>Dominican Republic: Floods and Landslides - No...</td>\n",
       "      <td>[]</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>https://reliefweb.int/disaster/fl-2023-000236-dom</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-11-28T19:32:06+00:00</td>\n",
       "      <td>At least 37060 people were displaced to safe a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15504</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL-2023-000236-DOM</td>\n",
       "      <td>Dominican Rep.</td>\n",
       "      <td>Dominican Republic: Floods and Landslides - No...</td>\n",
       "      <td>[]</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>https://reliefweb.int/disaster/fl-2023-000236-dom</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-11-28T19:32:06+00:00</td>\n",
       "      <td>A total of 4 shelters were opened, housing 158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15505</th>\n",
       "      <td>disaster summary</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL-2023-000236-DOM</td>\n",
       "      <td>Dominican Rep.</td>\n",
       "      <td>Dominican Republic: Floods and Landslides - No...</td>\n",
       "      <td>[]</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>https://reliefweb.int/disaster/fl-2023-000236-dom</td>\n",
       "      <td>According to [information from 22 November], 7...</td>\n",
       "      <td>reliefweb</td>\n",
       "      <td>2023-11-28T19:32:06+00:00</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            record_type   status  source_url            glide_id  \\\n",
       "0      disaster summary  ongoing         NaN  EP-2013-000175-COD   \n",
       "1      disaster summary  ongoing         NaN  EP-2013-000175-COD   \n",
       "2      disaster summary  ongoing         NaN  EP-2013-000175-COD   \n",
       "3      disaster summary  ongoing         NaN  EP-2013-000175-COD   \n",
       "4      disaster summary  ongoing         NaN  EP-2013-000175-COD   \n",
       "...                 ...      ...         ...                 ...   \n",
       "15501  disaster summary  ongoing         NaN  FL-2023-000236-DOM   \n",
       "15502  disaster summary  ongoing         NaN  FL-2023-000236-DOM   \n",
       "15503  disaster summary  ongoing         NaN  FL-2023-000236-DOM   \n",
       "15504  disaster summary  ongoing         NaN  FL-2023-000236-DOM   \n",
       "15505  disaster summary  ongoing         NaN  FL-2023-000236-DOM   \n",
       "\n",
       "      source_level_country                                       source_title  \\\n",
       "0                 DR Congo  DR Congo: Cholera and Measles Outbreaks - Jan ...   \n",
       "1                 DR Congo  DR Congo: Cholera and Measles Outbreaks - Jan ...   \n",
       "2                 DR Congo  DR Congo: Cholera and Measles Outbreaks - Jan ...   \n",
       "3                 DR Congo  DR Congo: Cholera and Measles Outbreaks - Jan ...   \n",
       "4                 DR Congo  DR Congo: Cholera and Measles Outbreaks - Jan ...   \n",
       "...                    ...                                                ...   \n",
       "15501       Dominican Rep.  Dominican Republic: Floods and Landslides - No...   \n",
       "15502       Dominican Rep.  Dominican Republic: Floods and Landslides - No...   \n",
       "15503       Dominican Rep.  Dominican Republic: Floods and Landslides - No...   \n",
       "15504       Dominican Rep.  Dominican Republic: Floods and Landslides - No...   \n",
       "15505       Dominican Rep.  Dominican Republic: Floods and Landslides - No...   \n",
       "\n",
       "      source_desc                               source_original_text  \\\n",
       "0              []  While the last major **cholera** outbreak in D...   \n",
       "1              []  While the last major **cholera** outbreak in D...   \n",
       "2              []  While the last major **cholera** outbreak in D...   \n",
       "3              []  While the last major **cholera** outbreak in D...   \n",
       "4              []  While the last major **cholera** outbreak in D...   \n",
       "...           ...                                                ...   \n",
       "15501          []  As of 21 November, severe rains and subsequent...   \n",
       "15502          []  According to [information from 22 November], 7...   \n",
       "15503          []  According to [information from 22 November], 7...   \n",
       "15504          []  According to [information from 22 November], 7...   \n",
       "15505          []  According to [information from 22 November], 7...   \n",
       "\n",
       "                                           reference_url  \\\n",
       "0      https://reliefweb.int/disaster/ep-2013-000175-cod   \n",
       "1      https://reliefweb.int/disaster/ep-2013-000175-cod   \n",
       "2      https://reliefweb.int/disaster/ep-2013-000175-cod   \n",
       "3      https://reliefweb.int/disaster/ep-2013-000175-cod   \n",
       "4      https://reliefweb.int/disaster/ep-2013-000175-cod   \n",
       "...                                                  ...   \n",
       "15501  https://reliefweb.int/disaster/fl-2023-000236-dom   \n",
       "15502  https://reliefweb.int/disaster/fl-2023-000236-dom   \n",
       "15503  https://reliefweb.int/disaster/fl-2023-000236-dom   \n",
       "15504  https://reliefweb.int/disaster/fl-2023-000236-dom   \n",
       "15505  https://reliefweb.int/disaster/fl-2023-000236-dom   \n",
       "\n",
       "                                                    text authoring_org  \\\n",
       "0      While the last major **cholera** outbreak in D...     reliefweb   \n",
       "1      While the last major **cholera** outbreak in D...     reliefweb   \n",
       "2      While the last major **cholera** outbreak in D...     reliefweb   \n",
       "3      While the last major **cholera** outbreak in D...     reliefweb   \n",
       "4      While the last major **cholera** outbreak in D...     reliefweb   \n",
       "...                                                  ...           ...   \n",
       "15501  As of 21 November, severe rains and subsequent...     reliefweb   \n",
       "15502  According to [information from 22 November], 7...     reliefweb   \n",
       "15503  According to [information from 22 November], 7...     reliefweb   \n",
       "15504  According to [information from 22 November], 7...     reliefweb   \n",
       "15505  According to [information from 22 November], 7...     reliefweb   \n",
       "\n",
       "                   reported_date  \\\n",
       "0      2023-10-16T12:15:16+00:00   \n",
       "1      2023-10-16T12:15:16+00:00   \n",
       "2      2023-10-16T12:15:16+00:00   \n",
       "3      2023-10-16T12:15:16+00:00   \n",
       "4      2023-10-16T12:15:16+00:00   \n",
       "...                          ...   \n",
       "15501  2023-11-28T19:32:06+00:00   \n",
       "15502  2023-11-28T19:32:06+00:00   \n",
       "15503  2023-11-28T19:32:06+00:00   \n",
       "15504  2023-11-28T19:32:06+00:00   \n",
       "15505  2023-11-28T19:32:06+00:00   \n",
       "\n",
       "                                         string_sentence  \n",
       "0      While the last major cholera outbreak in DR Co...  \n",
       "1      intdisasterep2011000076cod), cholera is endemi...  \n",
       "2      Over the course of 2013, ongoing violence and ...  \n",
       "3      Between January and September, more than 21000...  \n",
       "4      During the same time frame, more than 74299 ca...  \n",
       "...                                                  ...  \n",
       "15501                                                  .  \n",
       "15502  According to information from 22 November, 741...  \n",
       "15503  At least 37060 people were displaced to safe a...  \n",
       "15504  A total of 4 shelters were opened, housing 158...  \n",
       "15505                                                  .  \n",
       "\n",
       "[15506 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(situation_reports)\n",
    "\n",
    "## Naive sentence split below is a bad idea....\n",
    "df['string_sentence'] = df['text'].astype(str).apply(lambda x: string_preprocess(x).split('.'))\n",
    "df = df.explode('string_sentence')\n",
    "df['string_sentence'] = df['string_sentence'].apply(lambda x: x.strip() + '.')\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e67eccd-1929-4b85-92b5-a419e39596a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over the last 24 hours, 49 deaths were reported, the majority in Sindh, taking the death toll to 777 since mid-June, and 59,665 houses were damaged, taking the total damaged houses to 176,436, mostly in Sindh and Balochistan. According to National Disaster Management Authority (NDMA), as of 21 August, around 1,868,098 people were rescued, and 317,896 individuals are living in relief camps across the country. In Sindh, as of 20 August, 1,356,863 people are affected, with 309,944 households affected and around 495,381 are displaced due to floods in the province. ([ECHO, 22 Aug 2022](https://reliefweb.int/node/3878589/))\n",
      "\n",
      "Over the last 24 hours, 49 deaths were reported, the majority in Sindh, taking the death toll to 777 since midJune, and 59665 houses were damaged, taking the total damaged houses to 176436, mostly in Sindh and Balochistan.\n"
     ]
    }
   ],
   "source": [
    "idx = df.sample(1).index[0]\n",
    "print(df.loc[idx]['source_original_text']) #.tolist()[0]\n",
    "print()\n",
    "print(df.loc[idx]['string_sentence']) #.tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2173564f-a0b3-4856-8d9b-da2bb23a1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b build out initial dataframe\n",
    "df['spacy_doc'] = df['string_sentence'].apply(lambda x: nlp(x))\n",
    "df['lower_lemmas'] = df['spacy_doc'].apply(lambda x: [w.lemma_.lower() for w in x])\n",
    "\n",
    "df['string_sent_wo_parens'] = df['string_sentence'].apply(string_remove_parenthetical_content)\n",
    "df['spacy_wo_parens'] = df['string_sent_wo_parens'].apply(lambda x: nlp(x))\n",
    "df['wo_parens_lower_lemmas'] = df['spacy_wo_parens'].apply(lambda x: [w.lemma_.lower() for w in x])\n",
    "df['locations'] = df['spacy_doc'].apply(lambda doc: [e.text for e in doc.ents if e.label_ == 'GPE'])\n",
    "df['dates'] = df['spacy_doc'].apply(lambda doc: [e.text for e in doc.ents if e.label_ == 'DATE'])\n",
    "df['svot'] = df['spacy_wo_parens'].apply(lambda doc: list(extract.subject_verb_object_triples(doc)))\n",
    "df['future_verbs'] = df['spacy_doc'].apply(get_future_tense_verb)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a008d4d5-e16c-44c7-b277-c06ac4fb7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_and_add_indicator(df, indicators)\n",
    "df['record_type'] = df.apply(declare_primary_record_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "298fe78b-a2b8-4b98-beb5-9358c3cf0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_killed'] = df['spacy_wo_parens'][df['i_killed'] == 1].apply(obtain_killed_numeric_value)\n",
    "df['num_injured'] = df['spacy_wo_parens'][df['i_injured'] == 1].apply(obtain_injured_numeric_value)\n",
    "#df['num_killed'] = df['wo_parens_lower_lemmas'][df['i_killed'] == 1].apply(obtain_killed_numeric_value)\n",
    "#df['num_injured'] = df['wo_parens_lower_lemmas'][df['i_injured'] == 1].apply(obtain_injured_numeric_value)\n",
    "\n",
    "df['num_others'] = df['spacy_wo_parens'].apply(obtain_counted_noun_chunks)\n",
    "\n",
    "stop_noun_chunks = ['which','these','that','it','this']\n",
    "df['noun_chunks'] = df['spacy_wo_parens'].apply(lambda doc: [i for i in list(extract.noun_chunks(doc)) if i.text.lower() not in stop_noun_chunks])\n",
    "df['entities'] = df['spacy_wo_parens'].apply(obtain_all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b13c0f9-6439-4e74-9ec2-1a8f39ca18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid(x):\n",
    "    foo = uuid.uuid4().hex\n",
    "    return foo\n",
    "    \n",
    "df['sent_idx'] = df['string_sentence'].apply(generate_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5127499-47cc-4fbd-b701-8f25dcd6e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"c://temp//foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742feaf2-ca61-4a72-8337-d369b06cf3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use this as a repeatable-ish pattern for expanding on all the qualitative fields\n",
    "df_entities = df[['source_url','sent_idx','string_sentence','entities']][df['entities'].isna() == False].copy()\n",
    "df_entities['tmp'] = df_entities.apply(extract_entities, axis=1)\n",
    "df_entities = df_entities.drop(columns=['entities'])\n",
    "df_entities = df_entities.explode('tmp')\n",
    "df_entities['rec_type'] = 'ENTITY'\n",
    "df_entities = df_entities[df_entities['tmp'].isna() == False].copy() # added as a final cleanup to resolve stop ents\n",
    "df_entities[['rec_prefix','rec_key','rec_value']] = df_entities.apply(lambda x: split_key_value_in_df(x.tmp), axis=1)\n",
    "\n",
    "#now noun_chunks\n",
    "df_nouns = df[['source_url','sent_idx','string_sentence','noun_chunks']][df['noun_chunks'].isna() == False].copy()\n",
    "df_nouns['tmp'] = df_nouns.apply(extract_ncs, axis=1)\n",
    "df_nouns = df_nouns.drop(columns=['noun_chunks'])\n",
    "df_nouns = df_nouns.explode('tmp')\n",
    "df_nouns['rec_type'] = 'NOUN_SEQUENCE'\n",
    "df_nouns = df_nouns[df_nouns['tmp'].isna() == False].copy()\n",
    "df_nouns[['rec_prefix','rec_key','rec_value']] = df_nouns.apply(lambda x: split_key_value_in_df(x.tmp), axis=1)\n",
    "\n",
    "#quantitative values\n",
    "df_quants = df[['source_url','sent_idx','string_sentence','num_others']][df['num_others'] != ''].copy()\n",
    "df_quants['tmp'] = df_quants.apply(extract_numeric_key_values, axis=1)\n",
    "df_quants = df_quants.drop(columns=['num_others'])\n",
    "df_quants = df_quants.explode('tmp')\n",
    "df_quants['rec_type'] = 'QUANTIFIED_NOUN'\n",
    "df_quants[['rec_prefix','rec_key','rec_value']] = df_quants.apply(lambda x: split_key_value_in_df(x.tmp), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd7c520e-e9a4-4926-b6f2-a9e704056638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_url</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>string_sentence</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>rec_prefix</th>\n",
       "      <th>rec_key</th>\n",
       "      <th>rec_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ace705f240ef41a6a0eeef639e889796</td>\n",
       "      <td>Between January and September, more than 21000...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td>more than</td>\n",
       "      <td>21000</td>\n",
       "      <td>cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ace705f240ef41a6a0eeef639e889796</td>\n",
       "      <td>Between January and September, more than 21000...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>376</td>\n",
       "      <td>deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9359a8da82764f6a85432c89334211a8</td>\n",
       "      <td>During the same time frame, more than 74299 ca...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td>more than</td>\n",
       "      <td>74299</td>\n",
       "      <td>cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9359a8da82764f6a85432c89334211a8</td>\n",
       "      <td>During the same time frame, more than 74299 ca...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>1160</td>\n",
       "      <td>deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>d128deec6de54662bc659ddc45385314</td>\n",
       "      <td>In 2013, Katanga was the mostaffected province...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>13726</td>\n",
       "      <td>cholera cases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>abb40de965af414ba9d8db6f0360952a</td>\n",
       "      <td>The UN is also helping the Ministry of Health ...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>UN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>abb40de965af414ba9d8db6f0360952a</td>\n",
       "      <td>The UN is also helping the Ministry of Health ...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>Ministry of Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>abb40de965af414ba9d8db6f0360952a</td>\n",
       "      <td>The UN is also helping the Ministry of Health ...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>NORP</td>\n",
       "      <td>Haitian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>91f045ac8562476298374735a7d7ab99</td>\n",
       "      <td>There are concerns that damages to water manag...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>273434c211a246c492cc123ab3f864b3</td>\n",
       "      <td>According to information from 22 November, 741...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>22 November</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98639 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       source_url                          sent_idx  \\\n",
       "3             NaN  ace705f240ef41a6a0eeef639e889796   \n",
       "3             NaN  ace705f240ef41a6a0eeef639e889796   \n",
       "4             NaN  9359a8da82764f6a85432c89334211a8   \n",
       "4             NaN  9359a8da82764f6a85432c89334211a8   \n",
       "6             NaN  d128deec6de54662bc659ddc45385314   \n",
       "...           ...                               ...   \n",
       "15499         NaN  abb40de965af414ba9d8db6f0360952a   \n",
       "15499         NaN  abb40de965af414ba9d8db6f0360952a   \n",
       "15499         NaN  abb40de965af414ba9d8db6f0360952a   \n",
       "15500         NaN  91f045ac8562476298374735a7d7ab99   \n",
       "15502         NaN  273434c211a246c492cc123ab3f864b3   \n",
       "\n",
       "                                         string_sentence         rec_type  \\\n",
       "3      Between January and September, more than 21000...  QUANTIFIED_NOUN   \n",
       "3      Between January and September, more than 21000...  QUANTIFIED_NOUN   \n",
       "4      During the same time frame, more than 74299 ca...  QUANTIFIED_NOUN   \n",
       "4      During the same time frame, more than 74299 ca...  QUANTIFIED_NOUN   \n",
       "6      In 2013, Katanga was the mostaffected province...  QUANTIFIED_NOUN   \n",
       "...                                                  ...              ...   \n",
       "15499  The UN is also helping the Ministry of Health ...           ENTITY   \n",
       "15499  The UN is also helping the Ministry of Health ...           ENTITY   \n",
       "15499  The UN is also helping the Ministry of Health ...           ENTITY   \n",
       "15500  There are concerns that damages to water manag...           ENTITY   \n",
       "15502  According to information from 22 November, 741...           ENTITY   \n",
       "\n",
       "      rec_prefix rec_key           rec_value  \n",
       "3      more than   21000               cases  \n",
       "3                    376              deaths  \n",
       "4      more than   74299               cases  \n",
       "4                   1160              deaths  \n",
       "6                  13726       cholera cases  \n",
       "...          ...     ...                 ...  \n",
       "15499                ORG                  UN  \n",
       "15499                ORG  Ministry of Health  \n",
       "15499               NORP             Haitian  \n",
       "15500               DATE                2023  \n",
       "15502               DATE         22 November  \n",
       "\n",
       "[98639 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attributes = pd.concat([df_quants, df_nouns,df_entities])\n",
    "df_attributes = df_attributes.drop(columns=['tmp'])\n",
    "df_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb32b4be-93e9-4779-9713-22c3b825dcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101995, 54)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join base and attribute df\n",
    "df_joined = df.merge(df_attributes[['sent_idx','rec_type','rec_prefix','rec_key','rec_value']], left_on='sent_idx', right_on='sent_idx', how='left').copy()\n",
    "df_joined.explode('locations')\n",
    "df_joined['locations'] = df_joined['locations'].apply(lambda x: x[0] if len(x)==1 else '')\n",
    "df_joined.explode('dates')\n",
    "df_joined['dates'] = df_joined['dates'].apply(lambda x: x[0] if len(x)==1 else '')\n",
    "\n",
    "\n",
    "df_joined.explode('svot')\n",
    "df_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b187ad6a-2b12-4fad-b94b-1b231c1d4b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 24 April, flash floods killed 4 persons and damaged 53 houses (fully) and 212 houses (partially) in Dire Dawa on 25 April, river overflow damaged social infrastructure and affected livestock in Jinka town, SNNP and on 2526 April, flash floods affected 34507 households and displaced 15195 households in Erer, Sitti, Nogob and Korahe zones, Somali region.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glide_id</th>\n",
       "      <th>source_level_country</th>\n",
       "      <th>string_sentence</th>\n",
       "      <th>locations</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>rec_prefix</th>\n",
       "      <th>rec_key</th>\n",
       "      <th>rec_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>24</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15924</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>persons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>53</td>\n",
       "      <td>houses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15926</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>212</td>\n",
       "      <td>houses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15927</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15928</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>2526</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15929</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>34507</td>\n",
       "      <td>households</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15930</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td>displaced</td>\n",
       "      <td>15195</td>\n",
       "      <td>households</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15931</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>24 April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15932</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>flash floods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15933</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>4 persons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15934</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>53 houses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15935</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>212 houses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15936</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>Dire Dawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15937</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>25 April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15938</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>river overflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15939</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>social infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15940</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>Jinka town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15941</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>SNNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15942</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>2526 April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15943</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>flash floods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15944</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>34507 households</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15945</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>displaced 15195 households</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15946</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>Erer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15947</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>Nogob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15948</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>Korahe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15949</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>zones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15950</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>Somali region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15951</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>24 April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15952</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>25 April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15953</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15954</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Dawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15955</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>GPE</td>\n",
       "      <td>Jinka town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15956</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>GPE</td>\n",
       "      <td>Erer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15957</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>SNNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15958</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>Nogob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15959</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>Korahe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15960</th>\n",
       "      <td>FL-2020-000126-ETH</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>On 24 April, flash floods killed 4 persons and...</td>\n",
       "      <td></td>\n",
       "      <td>9fd4c79f6837427194db05edc2cbeddb</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>NORP</td>\n",
       "      <td>Somali</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 glide_id source_level_country  \\\n",
       "15923  FL-2020-000126-ETH             Ethiopia   \n",
       "15924  FL-2020-000126-ETH             Ethiopia   \n",
       "15925  FL-2020-000126-ETH             Ethiopia   \n",
       "15926  FL-2020-000126-ETH             Ethiopia   \n",
       "15927  FL-2020-000126-ETH             Ethiopia   \n",
       "15928  FL-2020-000126-ETH             Ethiopia   \n",
       "15929  FL-2020-000126-ETH             Ethiopia   \n",
       "15930  FL-2020-000126-ETH             Ethiopia   \n",
       "15931  FL-2020-000126-ETH             Ethiopia   \n",
       "15932  FL-2020-000126-ETH             Ethiopia   \n",
       "15933  FL-2020-000126-ETH             Ethiopia   \n",
       "15934  FL-2020-000126-ETH             Ethiopia   \n",
       "15935  FL-2020-000126-ETH             Ethiopia   \n",
       "15936  FL-2020-000126-ETH             Ethiopia   \n",
       "15937  FL-2020-000126-ETH             Ethiopia   \n",
       "15938  FL-2020-000126-ETH             Ethiopia   \n",
       "15939  FL-2020-000126-ETH             Ethiopia   \n",
       "15940  FL-2020-000126-ETH             Ethiopia   \n",
       "15941  FL-2020-000126-ETH             Ethiopia   \n",
       "15942  FL-2020-000126-ETH             Ethiopia   \n",
       "15943  FL-2020-000126-ETH             Ethiopia   \n",
       "15944  FL-2020-000126-ETH             Ethiopia   \n",
       "15945  FL-2020-000126-ETH             Ethiopia   \n",
       "15946  FL-2020-000126-ETH             Ethiopia   \n",
       "15947  FL-2020-000126-ETH             Ethiopia   \n",
       "15948  FL-2020-000126-ETH             Ethiopia   \n",
       "15949  FL-2020-000126-ETH             Ethiopia   \n",
       "15950  FL-2020-000126-ETH             Ethiopia   \n",
       "15951  FL-2020-000126-ETH             Ethiopia   \n",
       "15952  FL-2020-000126-ETH             Ethiopia   \n",
       "15953  FL-2020-000126-ETH             Ethiopia   \n",
       "15954  FL-2020-000126-ETH             Ethiopia   \n",
       "15955  FL-2020-000126-ETH             Ethiopia   \n",
       "15956  FL-2020-000126-ETH             Ethiopia   \n",
       "15957  FL-2020-000126-ETH             Ethiopia   \n",
       "15958  FL-2020-000126-ETH             Ethiopia   \n",
       "15959  FL-2020-000126-ETH             Ethiopia   \n",
       "15960  FL-2020-000126-ETH             Ethiopia   \n",
       "\n",
       "                                         string_sentence locations  \\\n",
       "15923  On 24 April, flash floods killed 4 persons and...             \n",
       "15924  On 24 April, flash floods killed 4 persons and...             \n",
       "15925  On 24 April, flash floods killed 4 persons and...             \n",
       "15926  On 24 April, flash floods killed 4 persons and...             \n",
       "15927  On 24 April, flash floods killed 4 persons and...             \n",
       "15928  On 24 April, flash floods killed 4 persons and...             \n",
       "15929  On 24 April, flash floods killed 4 persons and...             \n",
       "15930  On 24 April, flash floods killed 4 persons and...             \n",
       "15931  On 24 April, flash floods killed 4 persons and...             \n",
       "15932  On 24 April, flash floods killed 4 persons and...             \n",
       "15933  On 24 April, flash floods killed 4 persons and...             \n",
       "15934  On 24 April, flash floods killed 4 persons and...             \n",
       "15935  On 24 April, flash floods killed 4 persons and...             \n",
       "15936  On 24 April, flash floods killed 4 persons and...             \n",
       "15937  On 24 April, flash floods killed 4 persons and...             \n",
       "15938  On 24 April, flash floods killed 4 persons and...             \n",
       "15939  On 24 April, flash floods killed 4 persons and...             \n",
       "15940  On 24 April, flash floods killed 4 persons and...             \n",
       "15941  On 24 April, flash floods killed 4 persons and...             \n",
       "15942  On 24 April, flash floods killed 4 persons and...             \n",
       "15943  On 24 April, flash floods killed 4 persons and...             \n",
       "15944  On 24 April, flash floods killed 4 persons and...             \n",
       "15945  On 24 April, flash floods killed 4 persons and...             \n",
       "15946  On 24 April, flash floods killed 4 persons and...             \n",
       "15947  On 24 April, flash floods killed 4 persons and...             \n",
       "15948  On 24 April, flash floods killed 4 persons and...             \n",
       "15949  On 24 April, flash floods killed 4 persons and...             \n",
       "15950  On 24 April, flash floods killed 4 persons and...             \n",
       "15951  On 24 April, flash floods killed 4 persons and...             \n",
       "15952  On 24 April, flash floods killed 4 persons and...             \n",
       "15953  On 24 April, flash floods killed 4 persons and...             \n",
       "15954  On 24 April, flash floods killed 4 persons and...             \n",
       "15955  On 24 April, flash floods killed 4 persons and...             \n",
       "15956  On 24 April, flash floods killed 4 persons and...             \n",
       "15957  On 24 April, flash floods killed 4 persons and...             \n",
       "15958  On 24 April, flash floods killed 4 persons and...             \n",
       "15959  On 24 April, flash floods killed 4 persons and...             \n",
       "15960  On 24 April, flash floods killed 4 persons and...             \n",
       "\n",
       "                               sent_idx         rec_type rec_prefix  \\\n",
       "15923  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15924  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15925  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15926  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15927  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15928  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15929  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN              \n",
       "15930  9fd4c79f6837427194db05edc2cbeddb  QUANTIFIED_NOUN  displaced   \n",
       "15931  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15932  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15933  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15934  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15935  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15936  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15937  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15938  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15939  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15940  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15941  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15942  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15943  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15944  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15945  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15946  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15947  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15948  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15949  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15950  9fd4c79f6837427194db05edc2cbeddb    NOUN_SEQUENCE              \n",
       "15951  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15952  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15953  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15954  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15955  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15956  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15957  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15958  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15959  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "15960  9fd4c79f6837427194db05edc2cbeddb           ENTITY              \n",
       "\n",
       "          rec_key                   rec_value  \n",
       "15923          24                       April  \n",
       "15924           4                     persons  \n",
       "15925          53                      houses  \n",
       "15926         212                      houses  \n",
       "15927          25                       April  \n",
       "15928        2526                       April  \n",
       "15929       34507                  households  \n",
       "15930       15195                  households  \n",
       "15931  NOUN_CHUNK                    24 April  \n",
       "15932  NOUN_CHUNK                flash floods  \n",
       "15933  NOUN_CHUNK                   4 persons  \n",
       "15934  NOUN_CHUNK                   53 houses  \n",
       "15935  NOUN_CHUNK                  212 houses  \n",
       "15936  NOUN_CHUNK                   Dire Dawa  \n",
       "15937  NOUN_CHUNK                    25 April  \n",
       "15938  NOUN_CHUNK              river overflow  \n",
       "15939  NOUN_CHUNK       social infrastructure  \n",
       "15940  NOUN_CHUNK                  Jinka town  \n",
       "15941  NOUN_CHUNK                        SNNP  \n",
       "15942  NOUN_CHUNK                  2526 April  \n",
       "15943  NOUN_CHUNK                flash floods  \n",
       "15944  NOUN_CHUNK            34507 households  \n",
       "15945  NOUN_CHUNK  displaced 15195 households  \n",
       "15946  NOUN_CHUNK                        Erer  \n",
       "15947  NOUN_CHUNK                       Nogob  \n",
       "15948  NOUN_CHUNK                      Korahe  \n",
       "15949  NOUN_CHUNK                       zones  \n",
       "15950  NOUN_CHUNK               Somali region  \n",
       "15951        DATE                    24 April  \n",
       "15952        DATE                    25 April  \n",
       "15953        DATE                       April  \n",
       "15954      PERSON                        Dawa  \n",
       "15955         GPE                  Jinka town  \n",
       "15956         GPE                        Erer  \n",
       "15957         ORG                        SNNP  \n",
       "15958         ORG                       Nogob  \n",
       "15959         ORG                      Korahe  \n",
       "15960        NORP                      Somali  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidx = df_joined['sent_idx'].sample().tolist()[0]\n",
    "print(df_joined['string_sentence'][df_joined['sent_idx'] == sidx].tolist()[0])\n",
    "df_joined[['glide_id','source_level_country','string_sentence','locations','sent_idx','rec_type','rec_prefix','rec_key','rec_value']][df_joined['sent_idx'] == sidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89a70efb-e83a-4b58-b452-a504326c51fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>rec_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent_idx, rec_type, rec_value]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_foo = df_joined[['sent_idx','rec_type','rec_value']][df_joined['sent_idx'] == '17c67b978f934355b9e2ebacb1b76ba0']\n",
    "df_foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24621878-c86a-48bf-9bb8-40669c769570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(group):\n",
    "    noun_sequence_values = group[group['rec_type'] == 'NOUN_SEQUENCE']['rec_value']\n",
    "    entity_values = group[group['rec_type'] == 'ENTITY']['rec_value']\n",
    "    mask = ~((group['rec_type'] == 'NOUN_SEQUENCE') & (noun_sequence_values.isin(entity_values)))\n",
    "    return group[mask]\n",
    "\n",
    "# Apply the filtering operation grouped by 'sent_idx'\n",
    "df_joined = df_joined.groupby('sent_idx', group_keys=False).apply(filter_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efa9a724-daf4-49ac-949f-8463c2247eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"D://projects//_external_files//surveyor//files_for_dashboarding//disaster_reports_{generate_uuid(1)}.xlsx\"\n",
    "df_joined.to_excel(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d898c0f7-a693-49ea-b5b4-5c43da773fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2023, tm_mon=12, tm_mday=2, tm_hour=6, tm_min=1, tm_sec=17, tm_wday=5, tm_yday=336, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "print(time.localtime())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
