{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac893dee-0a35-4080-9773-5a062ada0fbf",
   "metadata": {},
   "source": [
    "# Begin the actual NLP work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dbeab91-739d-44c0-81f2-8b9c0949756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from textacy import extract\n",
    "\n",
    "from collections import defaultdict \n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13fe76aa-6c4d-4875-a536-5844ebfef3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitrep_preprocessed_file = \"D://projects//_external_files//surveyor//rw_sitrep_preprocessed//sitrep_preprocessed_d99e8ecc6fd74f849bd9163116c522ee.xlsx\"\n",
    "pcode_file = \"D://projects//_external_files//cod_files//combined_locations//locations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9a3e83d-ff47-4daf-87da-40eb2aa1e935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2023, tm_mon=12, tm_mday=12, tm_hour=6, tm_min=17, tm_sec=8, tm_wday=1, tm_yday=346, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf7f39-a913-4dd3-a11b-1ed061a3d279",
   "metadata": {},
   "source": [
    "## Load Location Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b633cc73-ea46-43ea-a45c-462c0190c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = pd.read_csv(pcode_file)\n",
    "\n",
    "def get_pcode_from_location(loc, country_prefix='XX', lang_code='all'):\n",
    "\n",
    "    if country_prefix != 'XX': #if the country prefix is set, limit search to that\n",
    "        df_loc = df_location[df_location['pcode_prefix'] == country_prefix]\n",
    "    else:\n",
    "        df_loc = df_location\n",
    "\n",
    "    if lang_code != 'all': #secondary filter - especially important to remove dupes with diff langs share the same script\n",
    "        df_loc = df_loc[df_loc['lang_code'] == lang_code]\n",
    "        \n",
    "    matches = df_loc['pcode'][df_loc['location_name'].str.lower() == loc.lower()].tolist()\n",
    "\n",
    "    #if the match fails, try again on the normalized name\n",
    "    if len(matches) == 0:\n",
    "        #remove common variations in names that can cause misses\n",
    "        n_loc = re.sub(r'[^a-zA-Z]', '', loc)\n",
    "\n",
    "        #this will cause problems for non-English.. so if then len is 0, exit\n",
    "        if len(n_loc) == 0:\n",
    "            return []\n",
    "            \n",
    "        matches = df_loc['pcode'][df_loc['location_normalized'].str.lower() == n_loc.lower()].tolist()\n",
    "        \n",
    "\n",
    "    #now check results\n",
    "    if len(matches) > 1:\n",
    "        #print(f\"more than 1 matches... likely due to different granularity of entities with the same name (ie. Herat City in Herat Province) {matches}\")\n",
    "        #print(f\"returning the lowest granularity match. {min(matches, key=len)}\")\n",
    "        #print(\"if the pcodes are all the same granularity.... you get the first element.\")\n",
    "        return min(matches, key=len)\n",
    "            \n",
    "        return matches[0]\n",
    "    elif len(matches) == 1:\n",
    "        return matches[0]\n",
    "\n",
    "    else:\n",
    "        #couldn't find a match, do a fuzzy search\n",
    "        compare_list = list(set(df_loc['location_name'].tolist()))\n",
    "        possible_matches=[]\n",
    "        for i in compare_list:\n",
    "            if fuzz.ratio(loc,i) > 70:\n",
    "                possible_matches.append(i)\n",
    "                print (f\"No exact match to '{loc}'. see if these alternative spellings are correct: {possible_matches}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "assert get_pcode_from_location('istanbul') == 'TUR034'\n",
    "\n",
    "def get_adm_lvl_from_pcode(pcode):\n",
    "    return list(set(df_location['adm_lvl'][df_location['pcode'] == pcode].tolist()))\n",
    "    \n",
    "def get_name_in_lang(pcode, lang='en'):\n",
    "    return list(set(df_location['location_name'][(df_location['pcode'] == pcode) & (df_location['lang_code'] == lang)].tolist()))\n",
    "\n",
    "def get_descendents_of(pcode, lang='en', include_self=True):\n",
    "    if include_self==True:\n",
    "        return df_location[df_location['pcode'].str.contains(pcode) & (df_location['lang_code'] == lang)]\n",
    "    else:\n",
    "        return df_location[df_location['pcode'].str.contains(pcode) & (df_location['lang_code'] == lang)\\\n",
    "        & (df_location['pcode'] != pcode)]\n",
    "\n",
    "def get_admin_chain(pcode, lang='en'):\n",
    "    split_pcode = df_location['split_pcode'][df_location['pcode'] == pcode].tolist()[0]\n",
    "    levels = split_pcode.split(\".\")\n",
    "    pc =''\n",
    "    admin_chain = []\n",
    "    #rebuild the pcode one level at a time\n",
    "    for i in levels:\n",
    "        pc = pc + i\n",
    "        admin_chain.append(df_location['location_name'][(df_location['pcode'] == pc) & (df_location['lang_code'] == lang)].tolist()[0])\n",
    "\n",
    "    return admin_chain\n",
    "\n",
    "def get_all_locations(lang_code='all'):\n",
    "\n",
    "    #return all unique location names\n",
    "    if lang_code == 'all':\n",
    "        return list(set(df_location['location_name'].to_list()))\n",
    "    else:\n",
    "        return list(set(df_location['location_name'][df_location['lang_code'] == lang_code].to_list()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be425ff0-6307-4bc1-ad74-6de9f333155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create patterns and add to the entity ruler to better find locations\n",
    "\n",
    "all_locs = get_all_locations(lang_code='en')\n",
    "gpes = []\n",
    "\n",
    "STOP_LOCS = ['of','can']\n",
    "all_locs = [e for e in all_locs if e.lower() not in STOP_LOCS]\n",
    "\n",
    "# create pattern rules for locations based on the COD files\n",
    "for l in all_locs:\n",
    "    token_sequence=[]\n",
    "    for token in l.split('\\s+'):\n",
    "        token_sequence.append({\"LOWER\":token.lower()})\n",
    "    x = {'label':'COD_GPE', 'pattern': token_sequence, 'id':get_pcode_from_location(l, lang_code='en')[0]}\n",
    "    gpes.append(x)\n",
    "    #print(get_pcode_from_location(l, lang_code='en'))\n",
    "\n",
    "ruler = nlp.add_pipe('entity_ruler', before='ner')\n",
    "ruler.add_patterns(gpes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d7ef1-036d-49de-b06e-136144e2c689",
   "metadata": {},
   "source": [
    "## Build the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08b10233-5b6b-4653-93e7-09e04540a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(sitrep_preprocessed_file)\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "502c73b3-663a-4278-9589-24fee6911212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_sentence_level(doc):\n",
    "    sentences = []\n",
    "    #print()\n",
    "    #print(doc)\n",
    "    for sent in doc.sents:\n",
    "        #print(sent)\n",
    "        #create new doc objects for each sentence and append to a list\n",
    "        doc_from_span = spacy.tokens.Doc(doc.vocab, words=[token.text for token in sent])\n",
    "        sentences.append(doc_from_span)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def expand_to_sentence_level(doc):\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text\n",
    "        if len(sent_text) > 20:\n",
    "            sentences.append(nlp(sent_text)) # horrendously inefficient but...\n",
    "    if len(sentences) == 0:\n",
    "        sentences.append(nlp(\"No content to return.\"))\n",
    "    return sentences\n",
    "\n",
    "# Function to increment by one for each idx_parad\n",
    "def generate_sent_id(group, new_column_name='idx_sent'):\n",
    "    group[new_column_name] = range(0, len(group))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79d10554-b22d-4688-9efa-8aeedc70086b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['record_type', 'source_url', 'glide_id', 'idx_para',\n",
       "       'source_level_country', 'source_title', 'source_desc',\n",
       "       'source_original_text', 'reference_url', 'text', 'authoring_org',\n",
       "       'reported_date', 'para_id', 'non_parenthetical_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6d6fe14-e59c-446d-ad86-5d5de365f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on ongoing for nowd\n",
    "df_sents = df.copy()\n",
    "df_sents['spacy_para_no_paren'] = df_sents['non_parenthetical_text'].apply(lambda x: nlp(x))\n",
    "df_sents['spacy_sent_no_paren'] = df_sents['spacy_para_no_paren'].apply(expand_to_sentence_level)\n",
    "df_sents = df_sents.explode('spacy_sent_no_paren')\n",
    "\n",
    "# Apply the function to the DataFrame using groupby on 'idx_para'\n",
    "df_sents = df_sents.groupby(['para_id','idx_para']).apply(generate_sent_id).reset_index(drop=True)\n",
    "df_sents = df_sents[['glide_id','source_level_country','authoring_org','para_id','idx_para','idx_sent','source_original_text','spacy_sent_no_paren','reported_date']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c8abfa-a3f8-4acd-ba48-3b73d336dad4",
   "metadata": {},
   "source": [
    "## Data Structure Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4c44777-c8cf-43f6-b74b-9221c887a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_indicators\n",
    "indicators = {\n",
    "    'i_people' : ['people','person','child','man','woman','civilian','colleague','fatality','individual']\n",
    "    ,'i_killed' : ['dead','fatal','die','kill','deceased','fatality','fatality','death','deaths'] #think about how to incorporate 2 co-existing terms \"648 people who lost their lives\"\n",
    "    ,'i_injured' : ['injure','wound','wounded','injured']\n",
    "    ,'i_damage' : ['damage','destroy','collapse']\n",
    "    ,'i_health_infrastructure' : ['hospital','surgery']\n",
    "    ,'i_education_infrastructure' : ['school','university']\n",
    "    ,'i_cash_xfer' : ['xx']\n",
    "    ,'i_wash' : ['sanitation','water','sewer','drain','drainage']\n",
    "    ,'i_shelter' : ['shelter','tent','camp','blanket']\n",
    "    ,'i_food' : ['food','cook','stove','feed','feed','nutrient','meal']\n",
    "    ,'i_logistic' : ['logistic','logistics']\n",
    "    ,'i_health' : ['health','medical','medicine']\n",
    "    ,'i_gender_vuln' : ['dignity','gender','pregnant','lactate','lactating']\n",
    "    ,'i_protection' : ['trauma','mental']\n",
    "    ,'i_response_capacity' : ['personnel']\n",
    "    ,'i_other_infrastructure' : ['communicate','radio','internet','telecommunication','electric','line']\n",
    "    ,'i_money' : ['grant','loan','finance','appeal','chf','fund']\n",
    "    ,'i_other' : ['biometric']\n",
    "    ,'i_problem' : ['challenge']\n",
    "    ,'i_demand_side' : ['need','demand','gap','priority', 'receive'] # note receive implies both supply and demand\n",
    "    ,'i_supply_side' : ['response','contribute','provide','source','address','deploy','receive'] # note receive implies both supply and demand\n",
    "\n",
    "    ,'i_assessments' : ['assess','assessment']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bd81d-6211-4cf1-bf4d-7102b10385ec",
   "metadata": {},
   "source": [
    "## Data Structure Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54b55d48-e7ac-4b3a-985e-697815d88da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gpe_entities(doc, adm_lvl='0'):\n",
    "    entities = []\n",
    "    admins = []\n",
    "    #COD_GPE\n",
    "    ents = list(extract.entities(doc))\n",
    "    if len(ents) < 1:\n",
    "        return None\n",
    "    else:\n",
    "        for e in ents:\n",
    "            if e.label_ == 'COD_GPE':\n",
    "                entities.append(e)\n",
    "\n",
    "        for e in entities:\n",
    "            pcode = get_pcode_from_location(e.text)\n",
    "            if (pcode is not None):\n",
    "                if (len(pcode) != 0):\n",
    "                    #print(pcode)\n",
    "                    admins.append(get_admin_chain(pcode)[adm_lvl])\n",
    "\n",
    "    admins = list(set(admins))\n",
    "    if len(admins) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return admins[0] \n",
    "\n",
    "df_sents['identified_country'] = df_sents['spacy_sent_no_paren'].apply(lambda x: extract_gpe_entities(x, adm_lvl=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee304255-1b77-47cb-a0d6-301d6d0afde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_add_indicator(df, indicators):\n",
    "    ind_counter = []\n",
    "    for ind in indicators:\n",
    "  \n",
    "        df[ind] = df['lower_lemmas'].apply(lambda x: 1 if len([w for w in x if w in indicators[ind]])>0 else 0)\n",
    "        ind_counter.append(ind)\n",
    "        #print(ind_counter)\n",
    "    df['i_count'] = df[ind_counter].sum(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f448d04-ae18-4daa-9dc0-746f63197f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sents['lower_lemmas'] = df_sents['spacy_sent_no_paren'].apply(lambda x: [w.lemma_.lower() for w in x])\n",
    "df_sents = find_and_add_indicator(df_sents, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3ccc1db-1817-4982-a232-15eecf17170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sents.to_excel(\"c://temp//training.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1f34b-7380-4e53-9818-1e0f901a6815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
